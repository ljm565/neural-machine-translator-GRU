# base
seed: 0
deterministic: True

# environment config
device: cpu     # examples: [0], [0,1], [1,2,3], cpu, mps... 

# project config
project: outputs/gru_w_attention
name: tatoeba

# model config
vocab_size: 10000
max_len: 128
num_layers: 3              # Number of each residual block's layer. A model consisting of a total of (num_layer * 2 * 3 + 2) layers will be created.
hidden_dim: 768
dropout: 0.1
use_attention: True

# data config
workers: 0               # Don't worry to set worker. The number of workers will be set automatically according to the batch size.
tatoeba_train: True      # if True, tatoeba will be loaded automatically.
tatoeba:
    path: data/
CUSTOM:
    train_data_path: null
    validation_data_path: null
    test_data_path: null

# train config
batch_size: 128
epochs: 100
lr: 1e-4
teacher_forcing_ratio: 1

# logging config
common: ['train_loss', 'validation_loss']
metrics: ['ppl', 'bleu2', 'bleu4', 'nist2', 'nist4']   # You can add more metrics after implements metric validation codes